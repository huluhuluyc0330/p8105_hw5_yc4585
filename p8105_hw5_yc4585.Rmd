---
title: "p8105_hw5_yc4585"
output: github_document
date: "2024-11-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
library(purrr)
```

# Problem 2
## set functions
```{r}
set.seed(1)
sim_mean_p = function(n = 30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)  
  )
  
  test_result = t.test(sim_data$x, mu = 0)  
  tidy_result = broom::tidy(test_result)  
  
  return(tibble(
    mu_hat = mean(sim_data$x),
    p_value = tidy_result$p.value
  ))
}
```
## Test of H: mu=0
```{r}
sim_results_df = 
  expand_grid(
    n = 30,
    mu=0,
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map2(n,mu,sim_mean_p)
  ) |> 
  unnest(estimate_df)

sim_results_df
```
## Test of H: mu={1,2,3,4,5,6}
```{r}
sim_multi_results_df = 
  expand_grid(
    n=30,
    mu = c(1,2,3,4,5,6),
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map2(n,mu,sim_mean_p)
  ) |> 
  unnest(estimate_df)

sim_multi_results_df
```
## Plot of Power of the t-test vs True Value of μ
```{r}
results_df <- bind_rows(sim_results_df,sim_multi_results_df)  
  power_data=
    results_df |>
    group_by(mu) |>
    summarise(power=mean(p_value<0.05))

power_plot=
  ggplot(power_data, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(title = "Power of the t-test vs True Value of μ",
       x = "True Value of μ",
       y = "Power (Proportion of Null Hypotheses Rejected)") +
  ylim(0,1)

power_plot
```

The relationship between the true value of mu and the power of the test is positively proportional. Since Power is the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.
A larger effect size (in our case, true value of mu) makes it easier to detect the effect, leading to higher power. From the plot we can see: as the true value of mu increases, the power of the test also increases. When the true value of mu is relatively small, the power increases rapidly. However, once the true value of mu approaches 4, the increase becomes more gradual, asymptotically approaching 1.
## plot of average mu hat vs true value of mu
```{r}
average_results = 
  results_df |>
  group_by(mu) |>
  summarise(
    average_mu_hat=mean(mu_hat)
  ) 

p_results = 
  results_df |>
  filter(p_value<0.05) |>
  group_by(mu) |>
  summarise(
    average_mu_hat_rejected=mean(mu_hat)
  ) 

average_plot=
    ggplot(data=average_results) +
      geom_line(data = average_results, aes(x = mu, y = average_mu_hat, color = "All Samples")) +
      geom_point(data = average_results, aes(x = mu, y = average_mu_hat, color = "All Samples")) +
      geom_line(data = p_results, aes(x = mu, y = average_mu_hat_rejected, color = "Rejected Null"),  linetype = "dashed") +
      geom_point(data = p_results, aes(x = mu, y = average_mu_hat_rejected, color = "Rejected Null")) +
      labs(title = "Average Estimatês of mu  vs True Valueofμ",
           x = "True Value of μ",
           y = "Average Estimate of μ̂") +
      scale_color_manual(name = "Sample Type", 
                         values = c("All Samples" = "blue", "Rejected Null" = "red")) +
      theme_minimal()

average_plot
```
Yes, the sample average of mu across test for which the null is rejected approximately does not equal to the true value of mu,especially in the cases where the true value of mu equals to 1 and 2, which could be because of selection bias.

# Problem3
## Import and clean data
```{r}
homicide_data=read.csv("./homicide-data.csv")

add_data =
  homicide_data |>
  mutate(city_state = paste(city, state, sep = ", ")) |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```
In the raw data of homicides in 50 large U.S. cities, there are 52,179 observations and 12 columns. The columns include the unique identifier (UID) of each crime, the reported date of the crime, the victim's first and last name, their race, age, and sex. Additionally, the dataset contains information about the city and state where the crime occurred, the latitude and longitude, and the disposition of the case.
## Proportion of unsolved homicides in Baltimore, MD
```{r}
MD_data = 
  add_data |>
  filter(city_state=="Baltimore, MD")

prop_test_result <- prop.test(x = MD_data$unsolved_homicides, 
                              n = MD_data$total_homicides)

tidy_result <- broom::tidy(prop_test_result)

estimated_proportion <- tidy_result |> pull(estimate)
conf_low <- tidy_result |> pull(conf.low)
conf_high <- tidy_result |> pull(conf.high)

list(
  estimated_proportion = estimated_proportion,
  conf_low = conf_low,
  conf_high = conf_high
)

```
## Proportion of unsolved homicides in each city
```{r}
run_prop_test <- function(unsolved, total) {
  prop_test_result <- prop.test(x = unsolved, n = total, correct = TRUE)
  tidy_result <- broom::tidy(prop_test_result)
  return(tidy_result)
}

all_city = add_data |>
  mutate(
    test_results = map2(unsolved_homicides, total_homicides, run_prop_test)
  ) |>
  unnest(test_results) |>
  mutate(
  confidence_interval = paste(conf.low, "-", conf.high))|>
  select(city_state, total_homicides, unsolved_homicides, estimate,confidence_interval, conf.low, conf.high)

all_city
```
## Plot of the estimates and CIs for each city
```{r}
all_city_plot=
ggplot(all_city, aes(x = reorder(city_state, estimate), 
                   y = estimate, 
                   ymin = conf.low, 
                   ymax = conf.high)) +
  geom_point() +  # Plot the estimate as a point
  geom_errorbar(width = 0.2) +  # Add error bars for the confidence intervals
  coord_flip() +  # Flip the coordinates to make the plot horizontal
  labs(
    x = "City", 
    y = "Proportion of Unsolved Homicides",
    title = "Proportion of Unsolved Homicides by City"
  ) 

all_city_plot
```

